{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "PSY 394U <b>Data Analytics with Python</b>, Spring 2018\n",
    "\n",
    "\n",
    "<img style=\"width: 400px; padding: 0px;\" src=\"https://github.com/sathayas/JupyterAnalyticsSpring2018/blob/master/images/Title_pics.png?raw=true\" alt=\"title pics\"/>\n",
    "\n",
    "</p>\n",
    "\n",
    "<p style=\"text-align:center; font-size:40px; margin-bottom: 30px;\"><b> Text processing </b></p>\n",
    "\n",
    "<p style=\"text-align:center; font-size:18px; margin-bottom: 32px;\"><b>April 12, 2018</b></p>\n",
    "\n",
    "<hr style=\"height:5px;border:none\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Example text data\n",
    "<hr style=\"height:1px;border:none\" />\n",
    "\n",
    "To start out with **NLTK** (**Natural Language Toolkit**), we first download some example data available for NLTK. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<NLTKIntro.py>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# downloading example text corpora \"book\"\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should open a downloader window\n",
    "\n",
    "<img style=\"width: 600px; padding: 0px;\" src=\"https://github.com/sathayas/JupyterAnalyticsSpring2018/blob/master/images/nltk_downloader.png?raw=true\" alt=\"NLTK downloader\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the downloader, please download the collection **book**. It contains a number of corpora (plural of corpus) used in the [NLTK book](http://www.nltk.org/book/). A corpus is a collection of texts. It may take a few minutes to download this collection. It takes about 420MB of disk space on your computer. \n",
    "\n",
    "Now, let's take a look at the *Gutenberg corpus* (**`gutenberg`** under **`nltk.corpus`**). The *Gutenberg* corpus includes a number of literary works, and can be used as example data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading the Gutenberg corpus\n",
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine the content of this corpus by the **`fileids()`** method, which produces a list of files as part of this corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, we will use Emma by Jane Austen (**`austen-emma.txt`**). We can simply load the raw text data with the **`raw`** method associated with the corpus `gutenberg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Emma by Jane Austen 1816]\n",
      "\n",
      "VOLUME I\n",
      "\n",
      "CHAPTER I\n",
      "\n",
      "\n",
      "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
      "and happy disposition, seemed to unite some of the best blessings\n",
      "of existence; and had lived nearly twenty-one years in the world\n",
      "with very little to distress or vex her.\n",
      "\n",
      "She was t\n"
     ]
    }
   ],
   "source": [
    "# loading the raw text\n",
    "emmaRawText = gutenberg.raw('austen-emma.txt')\n",
    "print(emmaRawText[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, we can load as a collection of words using the **`words`** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER', 'I', 'Emma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', ',', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', ';', 'and', 'had', 'lived', 'nearly', 'twenty', '-', 'one', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her', '.', 'She', 'was', 'the', 'youngest', 'of', 'the', 'two', 'daughters', 'of', 'a', 'most', 'affectionate', ',', 'indulgent', 'father', ';', 'and', 'had', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "# loading words\n",
    "emmaWords = gutenberg.words('austen-emma.txt')\n",
    "print(emmaWords[:80])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tokenizing sentences and words\n",
    "<hr style=\"height:1px;border:none\" />\n",
    "\n",
    "NLTK comes with **tokenizers** that let up split a text data into sentences or words. Here is an example. For this example, we will be downloading a text data set (The Adventures of Sherlock Holmes by Arthur Conan Doyle) from the [Project Gutenberg's main web site](http://www.gutenberg.org), a repository of free electronic books."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<Tokenize.py>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Loading The Adventures of Sherlock Holmes by Arthur Conan Doyle\n",
    "# from the Project Gutenberg\n",
    "from urllib import request\n",
    "url = \"http://www.gutenberg.org/ebooks/1661.txt.utf-8\"\n",
    "response = request.urlopen(url)\n",
    "rawText = response.read().decode('utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, **`rawText`** contains the entire book. Here is an excerpt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\r\n",
      "ADVENTURE I. A SCANDAL IN BOHEMIA\r\n",
      "\r\n",
      "I.\r\n",
      "\r\n",
      "To Sherlock Holmes she is always THE woman. I have seldom heard\r\n",
      "him mention her under any other name. In his eyes she eclipses\r\n",
      "and predominates the whole of her sex. It was not that he felt\r\n",
      "any emotion akin to love for Irene Adler. All emotions, and that\r\n",
      "one particularly, were abhorrent to his cold, precise but\r\n",
      "admirably balanced mind. He was, I take it, the most perfect\r\n",
      "reasoning and observing machine that the world has seen, but as a\r\n",
      "lover he would have placed himself in a false position. He never\r\n",
      "spoke of the softer passions, \n"
     ]
    }
   ],
   "source": [
    "print(rawText[1210:1800])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentence tokenizer, **`sent_tokenize`** breaks up a text into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['To Sherlock Holmes she is always THE woman.', 'I have seldom heard\\r\\nhim mention her under any other name.', 'In his eyes she eclipses\\r\\nand predominates the whole of her sex.']\n"
     ]
    }
   ],
   "source": [
    "# breaking up the raw text into sentences\n",
    "sentText = nltk.sent_tokenize(rawText)\n",
    "\n",
    "print(sentText[14:17])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word tokenizer, **`word_tokenize`** breaks up a text into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['To', 'Sherlock', 'Holmes', 'she', 'is', 'always', 'THE', 'woman', '.']\n",
      "['I', 'have', 'seldom', 'heard', 'him', 'mention', 'her', 'under', 'any', 'other', 'name', '.']\n",
      "['In', 'his', 'eyes', 'she', 'eclipses', 'and', 'predominates', 'the', 'whole', 'of', 'her', 'sex', '.']\n"
     ]
    }
   ],
   "source": [
    "# breaking up sentences into words\n",
    "print(nltk.word_tokenize(sentText[14]))\n",
    "print(nltk.word_tokenize(sentText[15]))\n",
    "print(nltk.word_tokenize(sentText[16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# breaking up the raw text into words\n",
    "wordText = nltk.word_tokenize(rawText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['To',\n",
       " 'Sherlock',\n",
       " 'Holmes',\n",
       " 'she',\n",
       " 'is',\n",
       " 'always',\n",
       " 'THE',\n",
       " 'woman',\n",
       " '.',\n",
       " 'I',\n",
       " 'have',\n",
       " 'seldom',\n",
       " 'heard',\n",
       " 'him',\n",
       " 'mention']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordText[220:235]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word frequency\n",
    "\n",
    "Now that the entire text is tokenized into words, we can determine word frequencies using the **`FreqDist`** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word frequency\n",
    "wordDist = nltk.FreqDist(wordText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the resulting **`wordDist`** is a dictionary-like object. You can get a list of most frequent words with the **`most_common`** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 7779), ('.', 5867), ('the', 5420), ('I', 2990), ('and', 2871), ('of', 2733), ('to', 2729), ('``', 2723), ('a', 2595), (\"''\", 2392), ('in', 1744), ('that', 1662), ('was', 1395), ('it', 1302), ('you', 1271), ('he', 1167), ('is', 1134), ('his', 1102), ('have', 907), ('my', 906), ('with', 849), ('had', 824), ('as', 780), ('which', 770), ('at', 743), ('?', 737), ('for', 716), ('not', 686), ('be', 642), ('me', 635)]\n"
     ]
    }
   ],
   "source": [
    "print(wordDist.most_common(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can get the frequency of a particular word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordDist['adventure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordDist['deduction']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating string data\n",
    "\n",
    "After tokenization, the text data can be handled as a collection of words. We can take advantage of various string manipulation methods and functions available in Python. For example, we can convert all words to lower case with the **`lower()`** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 7779), ('.', 5867), ('the', 5793), ('and', 3061), ('i', 2990), ('of', 2777), ('to', 2761), ('``', 2723), ('a', 2693), (\"''\", 2392), ('in', 1818), ('that', 1757), ('it', 1736), ('you', 1536), ('he', 1484), ('was', 1413), ('his', 1158), ('is', 1148), ('my', 999), ('have', 927), ('with', 877), ('as', 861), ('had', 833), ('at', 782), ('which', 776), ('for', 749), ('?', 737), ('not', 709), ('but', 648), ('be', 646)]\n"
     ]
    }
   ],
   "source": [
    "# word frequency after converting to lower case\n",
    "wordTextLower = [w.lower() for w in wordText]\n",
    "wordDistLower = nltk.FreqDist(wordTextLower)\n",
    "print(wordDistLower.most_common(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate a list of unique words in a word list by the **`set`** function. Here, we can extract some words satisfying certain conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'breckinridge\", \"'certainly\", \"'encyclopaedia\", \"'gesellschaft\", \"'hampshire\", \"'photography\", \"'pondicherry\", \"'precisely\", \"'remarkable\", \"'undoubtedly\"]\n"
     ]
    }
   ],
   "source": [
    "# just long words (10 characters or more)\n",
    "wordSetLower = sorted(set(wordTextLower))  # unique word list\n",
    "longWords = [w for w in wordSetLower if len(w)>9]\n",
    "print(longWords[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['absolutely', 'considerable', 'electronic', 'foundation', 'gutenberg-tm', 'interesting', 'photograph', 'stepfather', 'understand']\n"
     ]
    }
   ],
   "source": [
    "# long words appearing more than 20 times\n",
    "longFreqWords = [w for w in wordSetLower\n",
    "                 if (len(w)>9) and wordDistLower[w]>20]\n",
    "print(longFreqWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "1. **Word frequency, Emma**. Calculate the word frequency for the text data Emma, and generate a list of top 50 most frequent words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Part-of-speech tags\n",
    "<hr style=\"height:1px;border:none\" />\n",
    "\n",
    "**Part-of-speech** (**POS**) tags indicate categories of words with similar grammatical properties (e.g., verbs, nouns, adjectives, etc.). In NLTK, the **`pos_tag`** function can assign POS tags for each word. Here is an example. We focus on a sentence from Emma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<POS-Tag.py>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Even before Miss Taylor had ceased to hold the nominal\n",
      "office of governess, the mildness of her temper had hardly allowed\n",
      "her to impose any restraint; and the shadow of authority being\n",
      "now long passed away, they had been living together as friend and\n",
      "friend very mutually attached, and Emma doing just what she liked;\n",
      "highly esteeming Miss Taylor's judgment, but directed chiefly by\n",
      "her own.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# loading the Emma by Jane Austen\n",
    "from nltk.corpus import gutenberg\n",
    "emmaRawText = gutenberg.raw('austen-emma.txt')\n",
    "\n",
    "# tokenizing\n",
    "emmaSents = nltk.sent_tokenize(emmaRawText)\n",
    "print(emmaSents[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We break this up into words via `word_tokenize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emmaWords = nltk.word_tokenize(emmaSents[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emmaWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we assign POS tags to this sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Even', 'RB'), ('before', 'IN'), ('Miss', 'NNP'), ('Taylor', 'NNP'), ('had', 'VBD'), ('ceased', 'VBN'), ('to', 'TO'), ('hold', 'VB'), ('the', 'DT'), ('nominal', 'JJ'), ('office', 'NN'), ('of', 'IN'), ('governess', 'NN'), (',', ','), ('the', 'DT'), ('mildness', 'NN'), ('of', 'IN'), ('her', 'PRP$'), ('temper', 'NN'), ('had', 'VBD'), ('hardly', 'RB'), ('allowed', 'VBN'), ('her', 'PRP'), ('to', 'TO'), ('impose', 'VB'), ('any', 'DT'), ('restraint', 'NN'), (';', ':'), ('and', 'CC'), ('the', 'DT'), ('shadow', 'NN'), ('of', 'IN'), ('authority', 'NN'), ('being', 'VBG'), ('now', 'RB'), ('long', 'RB'), ('passed', 'VBN'), ('away', 'RB'), (',', ','), ('they', 'PRP'), ('had', 'VBD'), ('been', 'VBN'), ('living', 'VBG'), ('together', 'RB'), ('as', 'IN'), ('friend', 'NN'), ('and', 'CC'), ('friend', 'VB'), ('very', 'RB'), ('mutually', 'RB'), ('attached', 'VBN'), (',', ','), ('and', 'CC'), ('Emma', 'NNP'), ('doing', 'VBG'), ('just', 'RB'), ('what', 'WP'), ('she', 'PRP'), ('liked', 'VBD'), (';', ':'), ('highly', 'RB'), ('esteeming', 'VBG'), ('Miss', 'NNP'), ('Taylor', 'NNP'), (\"'s\", 'POS'), ('judgment', 'NN'), (',', ','), ('but', 'CC'), ('directed', 'VBD'), ('chiefly', 'NN'), ('by', 'IN'), ('her', 'PRP$'), ('own', 'JJ'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# POS tagging of an example sentence\n",
    "emmaTagged = nltk.pos_tag(emmaWords)\n",
    "print(emmaTagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may ask, \"what are those symbols, such as RB or IN?\" You can see a list of tags by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these tags, we can easily extract a certain category of words. For example, a tag for a verb usually starts with characters **`\"VB\"`**. So we can extract verbs only from the text data by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('had', 'VBD'), ('ceased', 'VBN'), ('hold', 'VB'), ('had', 'VBD'), ('allowed', 'VBN'), ('impose', 'VB'), ('being', 'VBG'), ('passed', 'VBN'), ('had', 'VBD'), ('been', 'VBN'), ('living', 'VBG'), ('friend', 'VB'), ('attached', 'VBN'), ('doing', 'VBG'), ('liked', 'VBD'), ('esteeming', 'VBG'), ('directed', 'VBD')]\n"
     ]
    }
   ],
   "source": [
    "# extracting verbs only (starting with VB)\n",
    "emmaVerbs = [w for w in emmaTagged if 'VB' in w[1]]\n",
    "print(emmaVerbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or adverbs (tags starting with **`\"RB\"`**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Even', 'RB'), ('hardly', 'RB'), ('now', 'RB'), ('long', 'RB'), ('away', 'RB'), ('together', 'RB'), ('very', 'RB'), ('mutually', 'RB'), ('just', 'RB'), ('highly', 'RB')]\n"
     ]
    }
   ],
   "source": [
    "# extracting adverbs only\n",
    "emmaAdv = [w for w in emmaTagged if 'RB' in w[1]]\n",
    "print(emmaAdv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or proper nouns (tags starting with **`\"NNP\"`**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Miss', 'NNP'), ('Taylor', 'NNP'), ('Emma', 'NNP'), ('Miss', 'NNP'), ('Taylor', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "# extracting proper nouns only\n",
    "emmaNNP = [w for w in emmaTagged if 'NNP' in w[1]]\n",
    "print(emmaNNP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "1. **POS frequencies**. In the POS-tagged sentence **`emmaTagged`**, count the number of\n",
    "  * Verbs: tags starting with **VB**\n",
    "  * Nouns: tags starting with **NN**\n",
    "  * Adjectives: tags starting with **JJ**\n",
    "  * Adverbs: tags starting with **RB**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Stop words and punctuation marks\n",
    "<hr style=\"height:1px;border:none\" />\n",
    "\n",
    "In the word frequency from an earlier example, you may have noticed that the most frequent words are actually punctuation marks (e.g., commas (,), periods (.), question marks (?), etc.) and a class of words known as stop words (e.g., \"the\", \"and\", \"of\", \"to\"). When analyzing text data, punctuation marks and stop words do not provide much information. Thus, we can eliminate these words.\n",
    "\n",
    "## Removing punctuation marks\n",
    "\n",
    "We can remove punctuation marks simply by using the **`isalpha()`** method for string data. For example, in the example sentence from Emma we saw earlier,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<StopwordsPunct.py>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Even before Miss Taylor had ceased to hold the nominal\n",
      "office of governess, the mildness of her temper had hardly allowed\n",
      "her to impose any restraint; and the shadow of authority being\n",
      "now long passed away, they had been living together as friend and\n",
      "friend very mutually attached, and Emma doing just what she liked;\n",
      "highly esteeming Miss Taylor's judgment, but directed chiefly by\n",
      "her own.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# loading the Emma by Jane Austen\n",
    "from nltk.corpus import gutenberg\n",
    "rawText = gutenberg.raw('austen-emma.txt')\n",
    "\n",
    "# tokenizing\n",
    "sentText = nltk.sent_tokenize(rawText)\n",
    "print(sentText[5])\n",
    "wordText = nltk.word_tokenize(sentText[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine whether each word consists of alphabets. If a word is anything other than alphabets, it will be eliminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# removing punctuation marks, making all words lower case\n",
    "wordDePunct = [w.lower() for w in wordText if w.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['even', 'before', 'miss', 'taylor', 'had', 'ceased', 'to', 'hold', 'the', 'nominal', 'office', 'of', 'governess', 'the', 'mildness', 'of', 'her', 'temper', 'had', 'hardly', 'allowed', 'her', 'to', 'impose', 'any', 'restraint', 'and', 'the', 'shadow', 'of', 'authority', 'being', 'now', 'long', 'passed', 'away', 'they', 'had', 'been', 'living', 'together', 'as', 'friend', 'and', 'friend', 'very', 'mutually', 'attached', 'and', 'emma', 'doing', 'just', 'what', 'she', 'liked', 'highly', 'esteeming', 'miss', 'taylor', 'judgment', 'but', 'directed', 'chiefly', 'by', 'her', 'own']\n"
     ]
    }
   ],
   "source": [
    "print(wordDePunct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing stop words\n",
    "\n",
    "Now, to remove stop words, we first have to import the corpus of stop words **`stopwords`**, and select that of English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# removing stopwords\n",
    "stop_words = set(stopwords.words('english'))  # stop words in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we examine whether each word is part of the set of stop words (**`stop_words`**). If a word is a stop word, that word is eliminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['even', 'miss', 'taylor', 'ceased', 'hold', 'nominal', 'office', 'governess', 'mildness', 'temper', 'hardly', 'allowed', 'impose', 'restraint', 'shadow', 'authority', 'long', 'passed', 'away', 'living', 'together', 'friend', 'friend', 'mutually', 'attached', 'emma', 'liked', 'highly', 'esteeming', 'miss', 'taylor', 'judgment', 'directed', 'chiefly']\n"
     ]
    }
   ],
   "source": [
    "wordNoStopwd = [w for w in wordDePunct if w not in stop_words]\n",
    "print(wordNoStopwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word frequency, before and after cleaning\n",
    "\n",
    "We shall see how removing punctuation marks and stop words alters the word frequency using the Sherlock Holmes example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<StopwordsFreq.py>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Loading The Adventures of Sherlock Holmes by Arthur Conan Doyle\n",
    "# from the Project Gutenberg\n",
    "from urllib import request\n",
    "url = \"http://www.gutenberg.org/ebooks/1661.txt.utf-8\"\n",
    "response = request.urlopen(url)\n",
    "rawText = response.read().decode('utf8')\n",
    "\n",
    "# tokenizing\n",
    "wordText = nltk.word_tokenize(rawText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a list of top 30 most frequent words, before the removal of punctuation marks and stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before text processing\n",
      ",              \t  7779\n",
      ".              \t  5867\n",
      "the            \t  5420\n",
      "I              \t  2990\n",
      "and            \t  2871\n",
      "of             \t  2733\n",
      "to             \t  2729\n",
      "``             \t  2723\n",
      "a              \t  2595\n",
      "''             \t  2392\n",
      "in             \t  1744\n",
      "that           \t  1662\n",
      "was            \t  1395\n",
      "it             \t  1302\n",
      "you            \t  1271\n",
      "he             \t  1167\n",
      "is             \t  1134\n",
      "his            \t  1102\n",
      "have           \t   907\n",
      "my             \t   906\n",
      "with           \t   849\n",
      "had            \t   824\n",
      "as             \t   780\n",
      "which          \t   770\n",
      "at             \t   743\n",
      "?              \t   737\n",
      "for            \t   716\n",
      "not            \t   686\n",
      "be             \t   642\n",
      "me             \t   635\n"
     ]
    }
   ],
   "source": [
    "# word frequency before removing punctuations and stop words\n",
    "print('Before text processing')\n",
    "wordFreqBefore = nltk.FreqDist(wordText)\n",
    "for iWord in wordFreqBefore.most_common(30):\n",
    "    print('%-15s\\t%6d' % iWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is a list of 30 most frequent words AFTER removing punctuation marks and stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After text processing\n",
      "said           \t   486\n",
      "upon           \t   467\n",
      "holmes         \t   466\n",
      "one            \t   374\n",
      "would          \t   333\n",
      "man            \t   303\n",
      "could          \t   288\n",
      "little         \t   269\n",
      "see            \t   232\n",
      "may            \t   210\n",
      "us             \t   184\n",
      "well           \t   176\n",
      "think          \t   174\n",
      "shall          \t   171\n",
      "must           \t   171\n",
      "know           \t   171\n",
      "come           \t   160\n",
      "time           \t   151\n",
      "came           \t   146\n",
      "two            \t   143\n",
      "door           \t   141\n",
      "back           \t   139\n",
      "room           \t   134\n",
      "face           \t   128\n",
      "might          \t   126\n",
      "matter         \t   125\n",
      "much           \t   121\n",
      "way            \t   116\n",
      "yes            \t   114\n",
      "house          \t   113\n"
     ]
    }
   ],
   "source": [
    "# removing punctuations and stopwords\n",
    "wordDePunct = [w.lower() for w in wordText if w.isalpha()]\n",
    "stop_words = set(stopwords.words('english'))  # stop words in English\n",
    "wordNoStopwd = [w for w in wordDePunct if w not in stop_words]\n",
    "\n",
    "\n",
    "# word frequency after removing punctuations and stop words\n",
    "print('After text processing')\n",
    "wordFreqAfter = nltk.FreqDist(wordNoStopwd)\n",
    "for iWord in wordFreqAfter.most_common(30):\n",
    "    print('%-15s\\t%6d' % iWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Stemming and lemmatizing\n",
    "<hr style=\"height:1px;border:none\" />\n",
    "\n",
    "Consider the word \"spam.\" It can be a noun in the singular or plural form (\"spam\" or \"spams\"). It can be a verb, thus conjugates depending on the context (e.g., \"spamming\", \"spammed\"). Or a new word can be created by adding a suffix (e.g., \"spammer\", \"spamize\", \"spamly\"). In terms of semantics, these are the same word, or closely related words. However, because they are spelled differently, they may be considered as distinct words. Two ways to get around that problem are **stemming** and **lemmatizing**.\n",
    "\n",
    "## Stemming\n",
    "\n",
    "**Stemming** extracts the root from a word. Here is a simple example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<Stemming.py>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# sample words\n",
    "sampleWords = ['spam', 'spams', 'spamming', 'spammed', 'spammer', 'spammers',\n",
    "               'spamize','spamly']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform stemming, we need to use the **`PorterStemmer`** transformation object under **`nltk.stem`**. We first need to define a stemming transformation object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stemmer object\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the object **`ps`** is defined, then we can use the **`stem`** method to determine the stem of a word. In our **`sampleWords`**, after stemming,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam\n",
      "spam\n",
      "spam\n",
      "spam\n",
      "spammer\n",
      "spammer\n",
      "spamiz\n",
      "spamli\n"
     ]
    }
   ],
   "source": [
    "for w in sampleWords:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that there are some common stems (**`spam`** and **`spammer`**). However, the other stems do not resemble real words (**`spamiz`** and **`spamli`**).\n",
    "\n",
    "Just for fun, we can apply stemming to our example sentence from Emma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading the Emma by Jane Austen\n",
    "from nltk.corpus import gutenberg\n",
    "rawText = gutenberg.raw('austen-emma.txt')\n",
    "\n",
    "# tokenizing\n",
    "sentText = nltk.sent_tokenize(rawText)\n",
    "wordText = nltk.word_tokenize(sentText[5])\n",
    "\n",
    "# removing punctuation marks & stop words, making all words lower case, \n",
    "wordDePunct = [w.lower() for w in wordText if w.isalpha()]\n",
    "stop_words = set(stopwords.words('english'))  # stop words in English\n",
    "wordNoStopwd = [w for w in wordDePunct if w not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the words before stemming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['even', 'miss', 'taylor', 'ceased', 'hold', 'nominal', 'office', 'governess', 'mildness', 'temper', 'hardly', 'allowed', 'impose', 'restraint', 'shadow', 'authority', 'long', 'passed', 'away', 'living', 'together', 'friend', 'friend', 'mutually', 'attached', 'emma', 'liked', 'highly', 'esteeming', 'miss', 'taylor', 'judgment', 'directed', 'chiefly']\n"
     ]
    }
   ],
   "source": [
    "# before stemming\n",
    "print(wordNoStopwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here are the words after stemming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['even', 'miss', 'taylor', 'ceas', 'hold', 'nomin', 'offic', 'gover', 'mild', 'temper', 'hardli', 'allow', 'impos', 'restraint', 'shadow', 'author', 'long', 'pass', 'away', 'live', 'togeth', 'friend', 'friend', 'mutual', 'attach', 'emma', 'like', 'highli', 'esteem', 'miss', 'taylor', 'judgment', 'direct', 'chiefli']\n"
     ]
    }
   ],
   "source": [
    "# after stemming\n",
    "wordStem = [ps.stem(w) for w in wordNoStopwd]\n",
    "print(wordStem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it works in some cases (e.g., `allowed` $\\rightarrow$ `allow` or `mutually` $\\rightarrow$ `mutual`). But some results are non-words (e.g., `ceased` $\\rightarrow$ `ceas` or `together` $\\rightarrow$ `togeth`). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing\n",
    "\n",
    "Unlike stemming, **lemmatizing** maps a word to its original form. For example,\n",
    "  * `cats` $\\rightarrow$ `cat`\n",
    "  * `cacti` $\\rightarrow$ `cactus`\n",
    "  * `geese` $\\rightarrow$ `goose`\n",
    "\n",
    "In NLTK, there is a transformation object **`WordNetLemmatizer`** under **`nltk.stem`**, that implements lemmatization. Here are some examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<Lemmatizing.py>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cats\n",
      "cacti\n",
      "geese\n",
      "rocks\n",
      "oxen\n",
      "ran\n",
      "spamming\n",
      "spammed\n",
      "spammer\n",
      "moves\n",
      "movement\n",
      "better\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# sample words\n",
    "sampleWords = ['cats','cacti','geese','rocks','oxen','ran','spamming',\n",
    "               'spammed','spammer','moves','movement','better']\n",
    "for w in sampleWords:\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "cactus\n",
      "goose\n",
      "rock\n",
      "ox\n",
      "ran\n",
      "spamming\n",
      "spammed\n",
      "spammer\n",
      "move\n",
      "movement\n",
      "better\n"
     ]
    }
   ],
   "source": [
    "# lemmatizer object\n",
    "lmt = WordNetLemmatizer()\n",
    "\n",
    "# lemmatized words\n",
    "for w in sampleWords:\n",
    "    print(lmt.lemmatize(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the lemmatizer assumes that the input word is a noun. If the word is not a noun, you can specify that with the **`pos`** parameter (**`'a'`** for adjectives, **`'v'`** for verbs, and **`'r'`** for adverbs). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n"
     ]
    }
   ],
   "source": [
    "# some non-noun words\n",
    "print(lmt.lemmatize('ran', pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n"
     ]
    }
   ],
   "source": [
    "print(lmt.lemmatize('better', pos='a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's go back to our sample sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading the Emma by Jane Austen\n",
    "from nltk.corpus import gutenberg\n",
    "rawText = gutenberg.raw('austen-emma.txt')\n",
    "\n",
    "# tokenizing\n",
    "sentText = nltk.sent_tokenize(rawText)\n",
    "wordText = nltk.word_tokenize(sentText[5])\n",
    "\n",
    "# removing punctuation marks & stop words, making all words lower case\n",
    "wordDePunct = [w.lower() for w in wordText if w.isalpha()]\n",
    "stop_words = set(stopwords.words('english'))  # stop words in English\n",
    "wordNoStopwd = [w for w in wordDePunct if w not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the words before lemmatizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['even', 'miss', 'taylor', 'ceased', 'hold', 'nominal', 'office', 'governess', 'mildness', 'temper', 'hardly', 'allowed', 'impose', 'restraint', 'shadow', 'authority', 'long', 'passed', 'away', 'living', 'together', 'friend', 'friend', 'mutually', 'attached', 'emma', 'liked', 'highly', 'esteeming', 'miss', 'taylor', 'judgment', 'directed', 'chiefly']\n"
     ]
    }
   ],
   "source": [
    "# before lemmatizing\n",
    "print(wordNoStopwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And after lemmatizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['even', 'miss', 'taylor', 'ceased', 'hold', 'nominal', 'office', 'governess', 'mildness', 'temper', 'hardly', 'allowed', 'impose', 'restraint', 'shadow', 'authority', 'long', 'passed', 'away', 'living', 'together', 'friend', 'friend', 'mutually', 'attached', 'emma', 'liked', 'highly', 'esteeming', 'miss', 'taylor', 'judgment', 'directed', 'chiefly']\n"
     ]
    }
   ],
   "source": [
    "# after lemmatizing\n",
    "wordLemma = [lmt.lemmatize(w) for w in wordNoStopwd]\n",
    "print(wordLemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, verbs are still not lemmatized. So we use information from POS tags to convert words according to their types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['even', 'miss', 'taylor', 'cease', 'hold', 'nominal', 'office', 'governess', 'mildness', 'temper', 'hardly', 'allow', 'impose', 'restraint', 'shadow', 'authority', 'long', 'pass', 'away', 'live', 'together', 'friend', 'friend', 'mutually', 'attach', 'emma', 'like', 'highly', 'esteem', 'miss', 'taylor', 'judgment', 'direct', 'chiefly']\n"
     ]
    }
   ],
   "source": [
    "# using POS tags\n",
    "wordPOS = nltk.pos_tag(wordText)\n",
    "# removing punctuation marks & stop words, making all words lower case, \n",
    "wordPOSDePunct = [(w[0].lower(), w[1]) for w in wordPOS if w[0].isalpha()]\n",
    "wordPOSNoStopwd = [w for w in wordPOSDePunct if w[0] not in stop_words]\n",
    "# initializing the lammatized word list\n",
    "wordPOSLemma = []\n",
    "for wPair in wordPOSNoStopwd:\n",
    "    if wPair[1][0] == 'J':   # i.e., adjectives\n",
    "        wordPOSLemma.append(lmt.lemmatize(wPair[0],pos='a'))\n",
    "    elif wPair[1][0] == 'V':  # i.e., verbs\n",
    "        wordPOSLemma.append(lmt.lemmatize(wPair[0],pos='v'))\n",
    "    elif 'RB' in wPair[1]:  # i.e., adverbs\n",
    "        wordPOSLemma.append(lmt.lemmatize(wPair[0],pos='r'))\n",
    "    else:\n",
    "        wordPOSLemma.append(lmt.lemmatize(wPair[0]))\n",
    "print(wordPOSLemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing and word frequency\n",
    "\n",
    "We can apply lemmatization on a larger text data. Let's examine the Sherlock Holmes example, and see how lemmatization affects word frequency counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<LemmatizingFreq.py>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# Loading The Adventures of Sherlock Holmes by Arthur Conan Doyle\n",
    "# from the Project Gutenberg\n",
    "from urllib import request\n",
    "url = \"http://www.gutenberg.org/ebooks/1661.txt.utf-8\"\n",
    "response = request.urlopen(url)\n",
    "rawText = response.read().decode('utf8')\n",
    "\n",
    "\n",
    "# tokenizing\n",
    "wordText = nltk.word_tokenize(rawText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the top 30 most frequent words, before lemmatization and other fancy text processing (removing punctuation marks and stop words, and turning all letters lower case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before text processing\n",
      ",              \t  7779\n",
      ".              \t  5867\n",
      "the            \t  5420\n",
      "I              \t  2990\n",
      "and            \t  2871\n",
      "of             \t  2733\n",
      "to             \t  2729\n",
      "``             \t  2723\n",
      "a              \t  2595\n",
      "''             \t  2392\n",
      "in             \t  1744\n",
      "that           \t  1662\n",
      "was            \t  1395\n",
      "it             \t  1302\n",
      "you            \t  1271\n",
      "he             \t  1167\n",
      "is             \t  1134\n",
      "his            \t  1102\n",
      "have           \t   907\n",
      "my             \t   906\n",
      "with           \t   849\n",
      "had            \t   824\n",
      "as             \t   780\n",
      "which          \t   770\n",
      "at             \t   743\n",
      "?              \t   737\n",
      "for            \t   716\n",
      "not            \t   686\n",
      "be             \t   642\n",
      "me             \t   635\n"
     ]
    }
   ],
   "source": [
    "# word frequency before doing fancy text processing stuff\n",
    "print('Before text processing')\n",
    "wordFreqBefore = nltk.FreqDist(wordText)\n",
    "for iWord in wordFreqBefore.most_common(30):\n",
    "    print('%-15s\\t%6d' % iWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now some fancy text processing and lemmatizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# removing punctuation marks & stop words, making all words lower case, \n",
    "wordDePunct = [w.lower() for w in wordText if w.isalpha()]\n",
    "stop_words = set(stopwords.words('english'))  # stop words in English\n",
    "wordNoStopwd = [w for w in wordDePunct if w not in stop_words]\n",
    "\n",
    "# Lemmatizing using POS tags\n",
    "lmt = WordNetLemmatizer()\n",
    "wordPOS = nltk.pos_tag(wordText)\n",
    "# removing punctuation marks & stop words, making all words lower case, \n",
    "wordPOSDePunct = [(w[0].lower(), w[1]) for w in wordPOS if w[0].isalpha()]\n",
    "wordPOSNoStopwd = [w for w in wordPOSDePunct if w[0] not in stop_words]\n",
    "# initializing the lammatized word list\n",
    "wordPOSLemma = []\n",
    "for wPair in wordPOSNoStopwd:\n",
    "    if wPair[1][0] == 'J':   # i.e., adjectives\n",
    "        wordPOSLemma.append(lmt.lemmatize(wPair[0],pos='a'))\n",
    "    elif wPair[1][0] == 'V':  # i.e., verbs\n",
    "        wordPOSLemma.append(lmt.lemmatize(wPair[0],pos='v'))\n",
    "    elif 'RB' in wPair[1]:  # i.e., adverbs\n",
    "        wordPOSLemma.append(lmt.lemmatize(wPair[0],pos='r'))\n",
    "    else:\n",
    "        wordPOSLemma.append(lmt.lemmatize(wPair[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall see how these steps affect the word frequency count. Here are the top 30 words on the processed and lemmatized text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After text processing\n",
      "say            \t   604\n",
      "upon           \t   467\n",
      "holmes         \t   466\n",
      "one            \t   383\n",
      "come           \t   349\n",
      "would          \t   333\n",
      "see            \t   328\n",
      "man            \t   303\n",
      "could          \t   288\n",
      "know           \t   271\n",
      "little         \t   269\n",
      "go             \t   248\n",
      "think          \t   244\n",
      "take           \t   215\n",
      "may            \t   210\n",
      "make           \t   209\n",
      "find           \t   201\n",
      "well           \t   196\n",
      "hand           \t   192\n",
      "u              \t   184\n",
      "look           \t   177\n",
      "shall          \t   171\n",
      "must           \t   171\n",
      "time           \t   169\n",
      "room           \t   161\n",
      "give           \t   156\n",
      "tell           \t   151\n",
      "door           \t   148\n",
      "matter         \t   145\n",
      "two            \t   143\n"
     ]
    }
   ],
   "source": [
    "# word frequency after fancy text processing stuff\n",
    "print('After text processing')\n",
    "wordFreqAfter = nltk.FreqDist(wordPOSLemma)\n",
    "for iWord in wordFreqAfter.most_common(30):\n",
    "    print('%-15s\\t%6d' % iWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "1. **Backward lemmatized word frequency**. Words that can be lemmatized to **`think`** include:\n",
    "```\n",
    "think, Think, thinks, Thinks, thought, Thought, thinking, Thinking\n",
    "```\n",
    "  Determine the frequency counts of these words in the Sherlock Holmes data set before text processing (i.e., in **`wordFreqBefore`**). What is the sum of the frequencies?\n",
    "\n",
    "2. **Verb or noun?** Among the possible words listed in the previous exercise, word **`thought`** is lemmatized to **`think`** if it is a verb, while it is lemmatized to **`thought`** if it is a noun. Determine how many of **`thought`**'s are verbs and how many of them are nouns. Likewise, the word **`thinking`** can be a verb or a noun. Determine how many of them are verbs and how many are nouns."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
